# -*- coding: utf-8 -*-
"""Results.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KtPOh08eQzz9RvdNDpGbwmJ7bHD0Awjg
"""

# google mount
from google.colab import drive
drive.mount('/content/drive')

"""Installation"""

# Installation
!pip install catboost

!pip install optuna

# import the library
import pandas as pd
import numpy as np

# Viz library
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OrdinalEncoder, MinMaxScaler

# Various feature selection technique
from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif

# Various imbalance technique
from imblearn.pipeline import Pipeline
from imblearn.over_sampling import ADASYN
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.under_sampling import TomekLinks

# Various models
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier , ExtraTreesClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier
import pickle


import optuna

# Evaluation metrics
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report, precision_recall_curve

df = pd.read_csv("/content/drive/MyDrive/MRP/Dataset/diabetes_012_health_indicators_BRFSS2015_categorical.csv")
df.head()

"""1. Read the csv file
2. Applied BMI categories and then perform ordinal encoding
"""

df.info()

df = df.astype(int)
df.info()

df.head()

# Step 1: Split into features and target
X = df.drop(columns='Diabetes_012')
y = df['Diabetes_012']

# Step 2: Split off Test set (20%)
X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)

# Step 3: Split remaining 80% into Train (62.5% of 80%) and Val1+Val2 (37.5% of 80%)
X_train, X_val_temp, y_train, y_val_temp = train_test_split(
    X_temp, y_temp, test_size=0.375, random_state=42, stratify=y_temp
)

# Step 4: Split Val1 and Val2 equally (each 18.75% of total)
X_val1, X_val2, y_val1, y_val2 = train_test_split(
    X_val_temp, y_val_temp, test_size=0.5, random_state=42, stratify=y_val_temp
)

# Check shapes
print("Train:", X_train.shape)
print("Val1 :", X_val1.shape)
print("Val2 :", X_val2.shape)
print("Test :", X_test.shape)

"""# With SMOTE as imbalance technique"""

# Step 2: Apply SMOTE to training data only
smote = SMOTE(random_state=42)
X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)

# Step 2 A: Check class balance after resampling
print("Before SMOTE:")
print(y_train.value_counts(normalize=True))
print("\nAfter SMOTE:")
print(pd.Series(y_train_sm).value_counts(normalize=True))

# Step 3: Feature Selection on SMOTE-applied training data

# Chi²
chi2_selector = SelectKBest(score_func=chi2, k='all').fit(X_train_sm, y_train_sm)
chi2_scores = pd.Series(chi2_selector.scores_, index=X_train.columns)


# Mutual Information
mi_selector = SelectKBest(score_func=mutual_info_classif, k='all').fit(X_train_sm, y_train_sm)
mi_scores = pd.Series(mi_selector.scores_, index=X_train.columns)


# Information Gain (Entropy-Based)
ig_model = DecisionTreeClassifier(criterion='entropy', random_state=42)
ig_model.fit(X_train_sm, y_train_sm)
ig_scores = pd.Series(ig_model.feature_importances_, index=X_train.columns)


# Normalize scores
scaler = MinMaxScaler()
chi2_scaled = pd.Series(scaler.fit_transform(chi2_scores.values.reshape(-1, 1)).flatten(), index=X_train.columns)
mi_scaled = pd.Series(scaler.fit_transform(mi_scores.values.reshape(-1, 1)).flatten(), index=X_train.columns)
ig_scaled = pd.Series(scaler.fit_transform(ig_scores.values.reshape(-1, 1)).flatten(), index=X_train.columns)

# Combine scores (equal weight)
#combined_score = chi2_scaled + mi_scaled + ig_scaled
#top_features = combined_score.sort_values(ascending=False).head(20)

# Create summary DataFrame
#top_features_df = pd.DataFrame({
#    "Feature": top_features.index,
#    "CombinedScore": top_features.values,
#    "Chi2_Score": chi2_scores[top_features.index].values,
#    "MI_Score": mi_scores[top_features.index].values,
#    "IG_Score": ig_scores[top_features.index].values
#})

# Create rank-based DataFrame without AvgRank
ranked_features_df = pd.DataFrame({
    "Feature": chi2_scores.index,
    "Chi2_Rank": chi2_scores.rank(ascending=False).astype(int),
    "MI_Rank": mi_scores.rank(ascending=False).astype(int),
    "IG_Rank": ig_scores.rank(ascending=False).astype(int)
})

# Total number of features for Borda Count
num_features = len(ranked_features_df)

# Convert ranks to Borda scores (higher is better)
borda_scores = pd.DataFrame({
    "Feature": ranked_features_df["Feature"],
    "Chi2_Score": num_features - ranked_features_df["Chi2_Rank"] + 1,
    "MI_Score": num_features - ranked_features_df["MI_Rank"] + 1,
    "IG_Score": num_features - ranked_features_df["IG_Rank"] + 1
})
borda_scores["Borda_Total"] = borda_scores[["Chi2_Score", "MI_Score", "IG_Score"]].sum(axis=1)

# Sort Borda Count results
balanced_BordaCount_df = borda_scores.sort_values("Borda_Total", ascending=False).reset_index(drop=True)
balanced_BordaCount_df

# Plotting the Borda Count results
plt.figure(figsize=(12, 8))
plt.barh(balanced_BordaCount_df['Feature'], balanced_BordaCount_df['Borda_Total'])
plt.title("Feature Importance on balanced dataset")
plt.xlabel("Borda Score")
plt.ylabel("Features")
plt.gca().invert_yaxis()  # Highest score at the top
plt.tight_layout()
plt.show()

# Create Rank-Score plot data
chi2_ranked = chi2_scaled.sort_values(ascending=False)
mi_ranked = mi_scaled.sort_values(ascending=False)
ig_ranked = ig_scaled.sort_values(ascending=False)

# Prepare data for plotting
plot_data = [
    (range(1, len(chi2_ranked)+1), chi2_ranked.values, 'Chi²'),
    (range(1, len(mi_ranked)+1), mi_ranked.values, 'Mutual Information'),
    (range(1, len(ig_ranked)+1), ig_ranked.values, 'Information Gain')
]

# Plot
plt.figure(figsize=(12, 7))
markers = ['o', 's', '^']
for (ranks, scores, label), marker in zip(plot_data, markers):
    plt.plot(ranks, scores, label=label, marker=marker)

plt.xlabel("Feature Rank")
plt.ylabel("Normalized Score")
plt.title("Rank-Score Graphs on balanced")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

#!pip install catboost

# Define the new feature sets for Top-K
feature_sets = {}
for k in range(2, 17, 2):  # 2, 4, 6, ..., 16
    key = f'Top{k}'
    feature_sets[key] = balanced_BordaCount_df['Feature'].head(k).tolist()

# Define models
models = {
    'LogisticRegression': Pipeline([
        ('scaler', MinMaxScaler()),
        ('model', LogisticRegression(C=1, class_weight='balanced', max_iter=1000))
    ]),
    'RandomForest': RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42),
    'LightGBM': LGBMClassifier(n_estimators=100, is_unbalance =  True, random_state=42),
    'CatBoost': CatBoostClassifier(depth=4, verbose=0, class_weights = [1, 10, 5],  random_state=42),
    'ExtraTrees': ExtraTreesClassifier(n_estimators=100, class_weight='balanced', random_state=42)
}

# Store results
results = []

# Evaluate each model on each feature set
for name, features in feature_sets.items():
    X_train_k = X_train_sm[features]
    X_val_k = X_val1[features]

    for model_name, model in models.items():
        model.fit(X_train_k, y_train_sm)
        preds = model.predict(X_val_k)

        # Calculate metrics
        # acc = accuracy_score(y_val1, preds)
        # prec = precision_score(y_val1, preds, average='weighted', zero_division=0)
        # rec = recall_score(y_val1, preds, average='weighted', zero_division=0)
        # f1 = f1_score(y_val1, preds, average='weighted', zero_division=0)

        # If possible, get prediction probabilities for AUC
        try:
            probs = model.predict_proba(X_val_k)
            auc = roc_auc_score(y_val1, probs, multi_class='ovr', average='weighted')
        except:
            auc = np.nan

        '''results.append({
            'Model': model_name,
            'Feature_Set': name,
            'Accuracy': acc,
            'Precision': prec,
            'Recall': rec,
            'F1_Score': f1,
            'AUC': auc
        })'''
        macro_f1 = f1_score(y_val1, preds, average='macro', zero_division=0)
        recall = recall_score(y_val1, preds, average='macro', zero_division=0)
        results.append({
            'Model': model_name,
            'Feature_Set': name,
            'Recall': recall,
            'Macro_F1': macro_f1,
            'AUC': auc
        })


# Convert to DataFrame
balanced_results_df = pd.DataFrame(results)
balanced_results_df

# Define color palette
palette = sns.color_palette("tab10")

# Plot
plt.figure(figsize=(12, 6))
for i, model in enumerate(balanced_results_df['Model'].unique()):
    subset = balanced_results_df[balanced_results_df['Model'] == model]
    feature_counts = subset['Feature_Set'].str.extract('(\d+)').astype(int)
    plt.plot(
        feature_counts[0],
        subset['Macro_F1'],
        marker='o',
        label=model,
        color=palette[i]
    )

plt.title("Top-k feature with Macro F1 Score")
plt.xlabel("Top-k Features")
plt.ylabel("Macro F1 Score")
plt.legend(title="Model")
plt.grid(True)
plt.tight_layout()
plt.show()

# Set color palette
sns.set_palette("tab10")

# Prepare AUC plot
plt.figure(figsize=(12, 6))

# Convert feature set names like 'Top2', 'Top4' to numeric values for x-axis
balanced_results_df['Num_Features'] = balanced_results_df['Feature_Set'].str.extract('(\d+)').astype(int)

# Plot AUC for each model
for model in balanced_results_df['Model'].unique():
    model_data = balanced_results_df[balanced_results_df['Model'] == model]
    plt.plot(model_data['Num_Features'], model_data['AUC'], marker='o', label=model)

plt.title("Top-k feature with AUC Score")
plt.xlabel("Top-k features")
plt.ylabel("AUC Score")
plt.legend(title="Model")
plt.grid(True)
plt.tight_layout()
plt.show()

# Highlight top 10 features
colors = ['tab:blue' if i >= 10 else 'tab:orange' for i in range(len(balanced_BordaCount_df))]

plt.figure(figsize=(12, 8))
plt.barh(balanced_BordaCount_df['Feature'], balanced_BordaCount_df['Borda_Total'], color=colors)
plt.title("Feature Importance")
plt.xlabel("Borda Count (Combined Rank)")
plt.ylabel("Features")
plt.gca().invert_yaxis()  # Highest score at the top
plt.tight_layout()
plt.show()

# Assuming X_train_final, y_train_final, X_val2, y_val2, and feature_sets["Top10"] are defined
# Define top10
top10 = feature_sets['Top10']

# Combine training data (X_train + X_val1) if not already done
X_train_concat = pd.concat([X_train, X_val1])
y_train_concat = pd.concat([y_train, y_val1])

# Apply SMOTE
smote = SMOTE(random_state=42)
X_train_final, y_train_final = smote.fit_resample(X_train_concat, y_train_concat)

# Dictionary to store best Optuna trials
best_trials = {}

# Objective function for Optuna tuning
def objective(trial, model_name):
    if model_name == 'LogisticRegression':
        C = trial.suggest_loguniform('C', 1e-3, 1e2)
        model = Pipeline([
            ('scaler', MinMaxScaler()),
            ('clf', LogisticRegression(C=C, class_weight='balanced', max_iter=1000))
        ])

    elif model_name == 'RandomForest':
        n_estimators = trial.suggest_int('n_estimators', 50, 300)
        max_depth = trial.suggest_int('max_depth', 2, 20)
        model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, class_weight='balanced', random_state=42)

    elif model_name == 'LightGBM':
        num_leaves = trial.suggest_int('num_leaves', 20, 150)
        max_depth = trial.suggest_int('max_depth', 3, 12)
        learning_rate = trial.suggest_float('learning_rate', 1e-3, 0.3, log=True)
        model = LGBMClassifier(num_leaves=num_leaves, max_depth=max_depth, is_unbalance=True,
                               learning_rate=learning_rate, n_estimators=100,
                               random_state=42)

    elif model_name == 'CatBoost':
        depth = trial.suggest_int('depth', 3, 10)
        learning_rate = trial.suggest_float('learning_rate', 1e-3, 0.3, log=True)
        model = CatBoostClassifier(depth=depth, learning_rate=learning_rate, class_weights=[1, 10, 5],
                                   n_estimators=100, verbose=0, random_state=42, task_type='CPU')

    elif model_name == 'ExtraTrees':
        n_estimators = trial.suggest_int('n_estimators', 50, 200)
        max_depth = trial.suggest_int('max_depth', 2, 20)
        model = ExtraTreesClassifier(n_estimators=n_estimators, max_depth=max_depth,
                                     class_weight='balanced', random_state=42)

    model.fit(X_train_final[top10], y_train_final)
    preds = model.predict_proba(X_val2[top10])
    auc = roc_auc_score(y_val2, preds, multi_class='ovr', average='weighted')
    return auc

# Run optimization for all models
for model_name in ['LogisticRegression', 'RandomForest', 'LightGBM', 'CatBoost', 'ExtraTrees']:
    print(f"Optimizing {model_name}...")
    study = optuna.create_study(direction='maximize')
    study.optimize(lambda trial: objective(trial, model_name), n_trials=30, show_progress_bar=True)
    best_trials[model_name] = study.best_trial
    print(f"Best AUC for {model_name}: {study.best_value:.4f}")
    print(f"Best Params: {study.best_params}")

# Display the best hyperparameters and AUC scores
for model_name, trial in best_trials.items():
    print(f"Model: {model_name}")
    print(f"  Best AUC Score : {trial.value:.4f}")
    print(f"  Best Parameters: {trial.params}")
    print("-" * 50)

def train_model(model_name, params):
    if model_name == 'LogisticRegression':
        model = Pipeline([
            ('scaler', MinMaxScaler()),
            ('clf', LogisticRegression(C=params['C'], max_iter=1000, class_weight='balanced', random_state=42))
        ])
    elif model_name == 'RandomForest':
        model = RandomForestClassifier(n_estimators=params['n_estimators'], class_weight='balanced',
                                       max_depth=params['max_depth'], random_state=42)
    elif model_name == 'LightGBM':
        model = LGBMClassifier(num_leaves=params['num_leaves'], max_depth=params['max_depth'], is_unbalance=True,
                               learning_rate=params['learning_rate'], n_estimators=100, random_state=42)
    elif model_name == 'CatBoost':
        model = CatBoostClassifier(depth=params['depth'], learning_rate=params['learning_rate'], class_weights=[1,10,5],
                                   n_estimators=100, verbose=0, random_state=42, task_type='CPU')
    elif model_name == 'ExtraTrees':
        model = ExtraTreesClassifier(n_estimators=params['n_estimators'], max_depth=params['max_depth'],
                                     class_weight='balanced', random_state=42)
    else:
        raise ValueError(f"Unsupported model: {model_name}")

    model.fit(X_train_final[top10], y_train_final)
    return model

def find_best_threshold(probs, y_true, class_index=1):
    class_probs = probs[:, class_index]
    precisions, recalls, thresholds = precision_recall_curve(y_true == class_index, class_probs)
    f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-8)
    best_idx = np.argmax(f1_scores)
    best_threshold = thresholds[best_idx] if best_idx < len(thresholds) else 1.0
    print(f"Best threshold for class {class_index}: {best_threshold:.3f} with F1 score: {f1_scores[best_idx]:.4f}")

    plt.figure(figsize=(8,6))
    plt.plot(thresholds, precisions[:-1], label='Precision')
    plt.plot(thresholds, recalls[:-1], label='Recall')
    plt.plot(thresholds, f1_scores[:-1], label='F1 Score')
    plt.axvline(best_threshold, color='r', linestyle='--', label='Best Threshold')
    plt.xlabel('Threshold')
    plt.ylabel('Score')
    plt.title(f'Precision, Recall, F1 vs Threshold for class {class_index}')
    plt.legend()
    plt.show()

    return best_threshold

def evaluate_model_with_optimal_threshold(model_name, params):
    model = train_model(model_name, params)
    pred_probs = model.predict_proba(X_test[top10])

    # Find best threshold for class 1
    best_thresh = find_best_threshold(pred_probs, y_test, class_index=1)

    # Apply threshold tuning to predictions
    preds_argmax = np.argmax(pred_probs, axis=1)
    preds_class1 = (pred_probs[:, 1] >= best_thresh).astype(int)
    preds = preds_argmax.copy()
    preds[preds_class1 == 1] = 1

    # Compute metrics
    acc = accuracy_score(y_test, preds)
    prec = precision_score(y_test, preds, average='weighted', zero_division=0)
    rec = recall_score(y_test, preds, average='weighted')
    f1 = f1_score(y_test, preds, average='weighted')
    auc = roc_auc_score(y_test, pred_probs, multi_class='ovr', average='weighted')
    cm = confusion_matrix(y_test, preds)
    report = classification_report(y_test, preds)

    print(f"Final evaluation for {model_name} at threshold {best_thresh:.3f}:")
    print(f"  Accuracy: {acc:.4f}")
    print(f"  Precision (weighted): {prec:.4f}")
    print(f"  Recall (weighted): {rec:.4f}")
    print(f"  F1 Score (weighted): {f1:.4f}")
    print(f"  AUC (weighted, OVR): {auc:.4f}")
    print("  Confusion Matrix:")
    print(cm)
    print("  Classification Report:")
    print(report)
    print("-" * 50)

    # Save results
    results.append({
        'Model': model_name,
        'Best Threshold': best_thresh,
        'Accuracy': acc,
        'Precision': prec,
        'Recall': rec,
        'F1 Score': f1,
        'AUC': auc
    })

# Run for all models in best_trials
for model_name, trial in best_trials.items():
    evaluate_model_with_optimal_threshold(model_name, trial.params)

# Store best trials of models

# Save best_trials to file
with open('/content/drive/MyDrive/MRP/best_trials_smote.pkl', 'wb') as f:
    pickle.dump(best_trials, f)

# Load best_trials from file
with open('best_trials_smote.pkl', 'rb') as f:
    best_trials_t = pickle.load(f)

# Display the best hyperparameters and AUC scores
for model_name, trial in best_trials_t.items():
    print(f"Model: {model_name}")
    print(f"  Best AUC Score : {trial.value:.4f}")
    print(f"  Best Parameters: {trial.params}")
    print("-" * 50)

import pandas as pd

# Assuming results is a list of dicts collected during evaluation
SMOTE_results_df = pd.DataFrame(results)

# Optionally, save to CSV for later use
SMOTE_results_df.to_csv('/content/drive/MyDrive/MRP/SMOTE_evaluation_results.csv', index=False)

"""# With ADASYN as imbalance technique"""

adasyn = ADASYN(random_state=42)
X_train_sm, y_train_sm = adasyn.fit_resample(X_train, y_train)

print("Before ADASYN:")
print(y_train.value_counts(normalize=True))
print("\nAfter ADASYN:")
print(pd.Series(y_train_sm).value_counts(normalize=True))

# Chi²
chi2_selector = SelectKBest(score_func=chi2, k='all').fit(X_train_sm, y_train_sm)
chi2_scores = pd.Series(chi2_selector.scores_, index=X_train.columns)


# Mutual Information
mi_selector = SelectKBest(score_func=mutual_info_classif, k='all').fit(X_train_sm, y_train_sm)
mi_scores = pd.Series(mi_selector.scores_, index=X_train.columns)


# Information Gain (Entropy-Based)
ig_model = DecisionTreeClassifier(criterion='entropy', random_state=42)
ig_model.fit(X_train_sm, y_train_sm)
ig_scores = pd.Series(ig_model.feature_importances_, index=X_train.columns)


# Normalize scores
scaler = MinMaxScaler()
chi2_scaled = pd.Series(scaler.fit_transform(chi2_scores.values.reshape(-1, 1)).flatten(), index=X_train.columns)
mi_scaled = pd.Series(scaler.fit_transform(mi_scores.values.reshape(-1, 1)).flatten(), index=X_train.columns)
ig_scaled = pd.Series(scaler.fit_transform(ig_scores.values.reshape(-1, 1)).flatten(), index=X_train.columns)

# Combine scores (equal weight)
#combined_score = chi2_scaled + mi_scaled + ig_scaled
#top_features = combined_score.sort_values(ascending=False).head(20)

# Create summary DataFrame
#top_features_df = pd.DataFrame({
#    "Feature": top_features.index,
#    "CombinedScore": top_features.values,
#    "Chi2_Score": chi2_scores[top_features.index].values,
#    "MI_Score": mi_scores[top_features.index].values,
#    "IG_Score": ig_scores[top_features.index].values
#})

# Create rank-based DataFrame without AvgRank
ranked_features_df = pd.DataFrame({
    "Feature": chi2_scores.index,
    "Chi2_Rank": chi2_scores.rank(ascending=False).astype(int),
    "MI_Rank": mi_scores.rank(ascending=False).astype(int),
    "IG_Rank": ig_scores.rank(ascending=False).astype(int)
})

# Total number of features for Borda Count
num_features = len(ranked_features_df)

# Convert ranks to Borda scores (higher is better)
borda_scores = pd.DataFrame({
    "Feature": ranked_features_df["Feature"],
    "Chi2_Score": num_features - ranked_features_df["Chi2_Rank"] + 1,
    "MI_Score": num_features - ranked_features_df["MI_Rank"] + 1,
    "IG_Score": num_features - ranked_features_df["IG_Rank"] + 1
})
borda_scores["Borda_Total"] = borda_scores[["Chi2_Score", "MI_Score", "IG_Score"]].sum(axis=1)

# Sort Borda Count results
balanced_BordaCount_df = borda_scores.sort_values("Borda_Total", ascending=False).reset_index(drop=True)
balanced_BordaCount_df

# Plotting the Borda Count results
plt.figure(figsize=(12, 8))
plt.barh(balanced_BordaCount_df['Feature'], balanced_BordaCount_df['Borda_Total'])
plt.title("Feature Importance on balanced dataset")
plt.xlabel("Borda Score")
plt.ylabel("Features")
plt.gca().invert_yaxis()  # Highest score at the top
plt.tight_layout()
plt.show()

# Create Rank-Score plot data
chi2_ranked = chi2_scaled.sort_values(ascending=False)
mi_ranked = mi_scaled.sort_values(ascending=False)
ig_ranked = ig_scaled.sort_values(ascending=False)

# Prepare data for plotting
plot_data = [
    (range(1, len(chi2_ranked)+1), chi2_ranked.values, 'Chi²'),
    (range(1, len(mi_ranked)+1), mi_ranked.values, 'Mutual Information'),
    (range(1, len(ig_ranked)+1), ig_ranked.values, 'Information Gain')
]

# Plot
plt.figure(figsize=(12, 7))
markers = ['o', 's', '^']
for (ranks, scores, label), marker in zip(plot_data, markers):
    plt.plot(ranks, scores, label=label, marker=marker)

plt.xlabel("Feature Rank")
plt.ylabel("Normalized Score")
plt.title("Rank-Score Graphs on balanced")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

#!pip install catboost

# Define the new feature sets for Top-K
feature_sets = {}
for k in range(2, 17, 2):  # 2, 4, 6, ..., 16
    key = f'Top{k}'
    feature_sets[key] = balanced_BordaCount_df['Feature'].head(k).tolist()

# Define models
models = {
    'LogisticRegression': Pipeline([
        ('scaler', MinMaxScaler()),
        ('model', LogisticRegression(C=1, class_weight='balanced', max_iter=1000))
    ]),
    'RandomForest': RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42),
    'LightGBM': LGBMClassifier(n_estimators=100, is_unbalance =  True, random_state=42),
    'CatBoost': CatBoostClassifier(depth=4, verbose=0, class_weights = [1, 10, 5],  random_state=42),
    'ExtraTrees': ExtraTreesClassifier(n_estimators=100, class_weight='balanced', random_state=42)
}

# Store results
results = []

# Evaluate each model on each feature set
for name, features in feature_sets.items():
    X_train_k = X_train_sm[features]
    X_val_k = X_val1[features]

    for model_name, model in models.items():
        model.fit(X_train_k, y_train_sm)
        preds = model.predict(X_val_k)

        # Calculate metrics
        # acc = accuracy_score(y_val1, preds)
        # prec = precision_score(y_val1, preds, average='weighted', zero_division=0)
        # rec = recall_score(y_val1, preds, average='weighted', zero_division=0)
        # f1 = f1_score(y_val1, preds, average='weighted', zero_division=0)

        # If possible, get prediction probabilities for AUC
        try:
            probs = model.predict_proba(X_val_k)
            auc = roc_auc_score(y_val1, probs, multi_class='ovr', average='weighted')
        except:
            auc = np.nan

        '''results.append({
            'Model': model_name,
            'Feature_Set': name,
            'Accuracy': acc,
            'Precision': prec,
            'Recall': rec,
            'F1_Score': f1,
            'AUC': auc
        })'''
        macro_f1 = f1_score(y_val1, preds, average='macro', zero_division=0)
        recall = recall_score(y_val1, preds, average='macro', zero_division=0)
        results.append({
            'Model': model_name,
            'Feature_Set': name,
            'Recall': recall,
            'Macro_F1': macro_f1,
            'AUC': auc
        })


# Convert to DataFrame
balanced_results_df = pd.DataFrame(results)
balanced_results_df

# Define color palette
palette = sns.color_palette("tab10")

# Plot
plt.figure(figsize=(12, 6))
for i, model in enumerate(balanced_results_df['Model'].unique()):
    subset = balanced_results_df[balanced_results_df['Model'] == model]
    feature_counts = subset['Feature_Set'].str.extract('(\d+)').astype(int)
    plt.plot(
        feature_counts[0],
        subset['Macro_F1'],
        marker='o',
        label=model,
        color=palette[i]
    )

plt.title("Top-k feature with Macro F1 Score")
plt.xlabel("Top-k Features")
plt.ylabel("Macro F1 Score")
plt.legend(title="Model")
plt.grid(True)
plt.tight_layout()
plt.show()

# Set color palette
sns.set_palette("tab10")

# Prepare AUC plot
plt.figure(figsize=(12, 6))

# Convert feature set names like 'Top2', 'Top4' to numeric values for x-axis
balanced_results_df['Num_Features'] = balanced_results_df['Feature_Set'].str.extract('(\d+)').astype(int)

# Plot AUC for each model
for model in balanced_results_df['Model'].unique():
    model_data = balanced_results_df[balanced_results_df['Model'] == model]
    plt.plot(model_data['Num_Features'], model_data['AUC'], marker='o', label=model)

plt.title("Top-k feature with AUC Score")
plt.xlabel("Top-k features")
plt.ylabel("AUC Score")
plt.legend(title="Model")
plt.grid(True)
plt.tight_layout()
plt.show()

# Highlight top 10 features
colors = ['tab:blue' if i >= 10 else 'tab:orange' for i in range(len(balanced_BordaCount_df))]

plt.figure(figsize=(12, 8))
plt.barh(balanced_BordaCount_df['Feature'], balanced_BordaCount_df['Borda_Total'], color=colors)
plt.title("Feature Importance")
plt.xlabel("Borda Count (Combined Rank)")
plt.ylabel("Features")
plt.gca().invert_yaxis()  # Highest score at the top
plt.tight_layout()
plt.show()

# Assuming X_train_final, y_train_final, X_val2, y_val2, and feature_sets["Top10"] are defined
# Define top10
top10 = feature_sets['Top10']

# Combine training data (X_train + X_val1) if not already done
X_train_concat = pd.concat([X_train, X_val1])
y_train_concat = pd.concat([y_train, y_val1])

# Apply hybrid undersample

from imblearn.pipeline import Pipeline
from imblearn.under_sampling import RandomUnderSampler, TomekLinks

pipeline = Pipeline([
    ('rus', RandomUnderSampler(random_state=42)),
    ('tl', TomekLinks())
])

X_train_final, y_train_final = pipeline.fit_resample(X_train_concat, y_train_concat)


# Dictionary to store best Optuna trials
best_trials = {}

# Objective function for Optuna tuning
def objective(trial, model_name):
    if model_name == 'LogisticRegression':
        C = trial.suggest_loguniform('C', 1e-3, 1e2)
        model = Pipeline([
            ('scaler', MinMaxScaler()),
            ('clf', LogisticRegression(C=C, class_weight='balanced', max_iter=1000))
        ])

    elif model_name == 'RandomForest':
        n_estimators = trial.suggest_int('n_estimators', 50, 300)
        max_depth = trial.suggest_int('max_depth', 2, 20)
        model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, class_weight='balanced', random_state=42)

    elif model_name == 'LightGBM':
        num_leaves = trial.suggest_int('num_leaves', 20, 150)
        max_depth = trial.suggest_int('max_depth', 3, 12)
        learning_rate = trial.suggest_float('learning_rate', 1e-3, 0.3, log=True)
        model = LGBMClassifier(num_leaves=num_leaves, max_depth=max_depth, is_unbalance=True,
                               learning_rate=learning_rate, n_estimators=100,
                               random_state=42)

    elif model_name == 'CatBoost':
        depth = trial.suggest_int('depth', 3, 10)
        learning_rate = trial.suggest_float('learning_rate', 1e-3, 0.3, log=True)
        model = CatBoostClassifier(depth=depth, learning_rate=learning_rate, class_weights=[1, 10, 5],
                                   n_estimators=100, verbose=0, random_state=42, task_type='CPU')

    elif model_name == 'ExtraTrees':
        n_estimators = trial.suggest_int('n_estimators', 50, 200)
        max_depth = trial.suggest_int('max_depth', 2, 20)
        model = ExtraTreesClassifier(n_estimators=n_estimators, max_depth=max_depth,
                                     class_weight='balanced', random_state=42)

    model.fit(X_train_final[top10], y_train_final)
    preds = model.predict_proba(X_val2[top10])
    auc = roc_auc_score(y_val2, preds, multi_class='ovr', average='weighted')
    return auc

# Run optimization for all models
for model_name in ['LogisticRegression', 'RandomForest', 'LightGBM', 'CatBoost', 'ExtraTrees']:
    print(f"Optimizing {model_name}...")
    study = optuna.create_study(direction='maximize')
    study.optimize(lambda trial: objective(trial, model_name), n_trials=30, show_progress_bar=True)
    best_trials[model_name] = study.best_trial
    print(f"Best AUC for {model_name}: {study.best_value:.4f}")
    print(f"Best Params: {study.best_params}")

# Display the best hyperparameters and AUC scores
for model_name, trial in best_trials_t.items():
    print(f"Model: {model_name}")
    print(f"  Best AUC Score : {trial.value:.4f}")
    print(f"  Best Parameters: {trial.params}")
    print("-" * 50)

results = []
def train_model(model_name, params):
    if model_name == 'LogisticRegression':
        model = Pipeline([
            ('scaler', MinMaxScaler()),
            ('clf', LogisticRegression(C=params['C'], max_iter=1000, class_weight='balanced', random_state=42))
        ])
    elif model_name == 'RandomForest':
        model = RandomForestClassifier(n_estimators=params['n_estimators'], class_weight='balanced',
                                       max_depth=params['max_depth'], random_state=42)
    elif model_name == 'LightGBM':
        model = LGBMClassifier(num_leaves=params['num_leaves'], max_depth=params['max_depth'], is_unbalance=True,
                               learning_rate=params['learning_rate'], n_estimators=100, random_state=42)
    elif model_name == 'CatBoost':
        model = CatBoostClassifier(depth=params['depth'], learning_rate=params['learning_rate'], class_weights=[1,10,5],
                                   n_estimators=100, verbose=0, random_state=42, task_type='CPU')
    elif model_name == 'ExtraTrees':
        model = ExtraTreesClassifier(n_estimators=params['n_estimators'], max_depth=params['max_depth'],
                                     class_weight='balanced', random_state=42)
    else:
        raise ValueError(f"Unsupported model: {model_name}")

    model.fit(X_train_final[top10], y_train_final)
    return model

def find_best_threshold(probs, y_true, class_index=1):
    class_probs = probs[:, class_index]
    precisions, recalls, thresholds = precision_recall_curve(y_true == class_index, class_probs)
    f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-8)
    best_idx = np.argmax(f1_scores)
    best_threshold = thresholds[best_idx] if best_idx < len(thresholds) else 1.0
    print(f"Best threshold for class {class_index}: {best_threshold:.3f} with F1 score: {f1_scores[best_idx]:.4f}")

    plt.figure(figsize=(8,6))
    plt.plot(thresholds, precisions[:-1], label='Precision')
    plt.plot(thresholds, recalls[:-1], label='Recall')
    plt.plot(thresholds, f1_scores[:-1], label='F1 Score')
    plt.axvline(best_threshold, color='r', linestyle='--', label='Best Threshold')
    plt.xlabel('Threshold')
    plt.ylabel('Score')
    plt.title(f'Precision, Recall, F1 vs Threshold for class {class_index}')
    plt.legend()
    plt.show()

    return best_threshold

def evaluate_model_with_optimal_threshold(model_name, params):
    model = train_model(model_name, params)
    pred_probs = model.predict_proba(X_test[top10])

    # Find best threshold for class 1
    best_thresh = find_best_threshold(pred_probs, y_test, class_index=1)

    # Apply threshold tuning to predictions
    preds_argmax = np.argmax(pred_probs, axis=1)
    preds_class1 = (pred_probs[:, 1] >= best_thresh).astype(int)
    preds = preds_argmax.copy()
    preds[preds_class1 == 1] = 1

    # Compute metrics
    acc = accuracy_score(y_test, preds)
    prec = precision_score(y_test, preds, average='weighted', zero_division=0)
    rec = recall_score(y_test, preds, average='weighted')
    f1 = f1_score(y_test, preds, average='weighted')
    auc = roc_auc_score(y_test, pred_probs, multi_class='ovr', average='weighted')
    cm = confusion_matrix(y_test, preds)
    report = classification_report(y_test, preds)

    print(f"Final evaluation for {model_name} at threshold {best_thresh:.3f}:")
    print(f"  Accuracy: {acc:.4f}")
    print(f"  Precision (weighted): {prec:.4f}")
    print(f"  Recall (weighted): {rec:.4f}")
    print(f"  F1 Score (weighted): {f1:.4f}")
    print(f"  AUC (weighted, OVR): {auc:.4f}")
    print("  Confusion Matrix:")
    print(cm)
    print("  Classification Report:")
    print(report)
    print("-" * 50)

    # Save results
    results.append({
        'Model': model_name,
        'Best Threshold': best_thresh,
        'Accuracy': acc,
        'Precision': prec,
        'Recall': rec,
        'F1 Score': f1,
        'AUC': auc
    })

# Run for all models in best_trials
for model_name, trial in best_trials_t.items():
    evaluate_model_with_optimal_threshold(model_name, trial.params)

# Store best trials of the model
# Save best_trials to file
with open('/content/drive/MyDrive/MRP/best_trials_adasyn.pkl', 'wb') as f:
    pickle.dump(best_trials, f)

# Load best_trials from file
with open('/content/drive/MyDrive/MRP/best_trials_adasyn.pkl', 'rb') as f:
    best_trials_t = pickle.load(f)

# Display the best hyperparameters and AUC scores
for model_name, trial in best_trials_t.items():
    print(f"Model: {model_name}")
    print(f"  Best AUC Score : {trial.value:.4f}")
    print(f"  Best Parameters: {trial.params}")
    print("-" * 50)

# Assuming results is a list of dicts collected during evaluation
ADASYN_results_df = pd.DataFrame(results)

# Optionally, save to CSV for later use
ADASYN_results_df.to_csv('/content/drive/MyDrive/MRP/ADASYN_evaluation_results.csv', index=False)

ADASYN_results_df

"""# With hybrid oversampling"""

# Create the hybrid pipeline: SMOTE then ADASYN
from imblearn.pipeline import Pipeline
hybrid_sampler = Pipeline([
    ('smote', SMOTE(random_state=42)),
    ('adasyn', ADASYN(random_state=42))
])
X_train_sm, y_train_sm = hybrid_sampler.fit_resample(X_train, y_train)

# Step 2 A: Check class balance after resampling
print("Before hybrid:")
print(y_train.value_counts(normalize=True))
print("\nAfter hybrid:")
print(pd.Series(y_train_sm).value_counts(normalize=True))

# Step 3: Feature Selection on SMOTE-applied training data

# Chi²
chi2_selector = SelectKBest(score_func=chi2, k='all').fit(X_train_sm, y_train_sm)
chi2_scores = pd.Series(chi2_selector.scores_, index=X_train.columns)


# Mutual Information
mi_selector = SelectKBest(score_func=mutual_info_classif, k='all').fit(X_train_sm, y_train_sm)
mi_scores = pd.Series(mi_selector.scores_, index=X_train.columns)


# Information Gain (Entropy-Based)
ig_model = DecisionTreeClassifier(criterion='entropy', random_state=42)
ig_model.fit(X_train_sm, y_train_sm)
ig_scores = pd.Series(ig_model.feature_importances_, index=X_train.columns)


# Normalize scores
scaler = MinMaxScaler()
chi2_scaled = pd.Series(scaler.fit_transform(chi2_scores.values.reshape(-1, 1)).flatten(), index=X_train.columns)
mi_scaled = pd.Series(scaler.fit_transform(mi_scores.values.reshape(-1, 1)).flatten(), index=X_train.columns)
ig_scaled = pd.Series(scaler.fit_transform(ig_scores.values.reshape(-1, 1)).flatten(), index=X_train.columns)

# Combine scores (equal weight)
#combined_score = chi2_scaled + mi_scaled + ig_scaled
#top_features = combined_score.sort_values(ascending=False).head(20)

# Create summary DataFrame
#top_features_df = pd.DataFrame({
#    "Feature": top_features.index,
#    "CombinedScore": top_features.values,
#    "Chi2_Score": chi2_scores[top_features.index].values,
#    "MI_Score": mi_scores[top_features.index].values,
#    "IG_Score": ig_scores[top_features.index].values
#})

# Create rank-based DataFrame without AvgRank
ranked_features_df = pd.DataFrame({
    "Feature": chi2_scores.index,
    "Chi2_Rank": chi2_scores.rank(ascending=False).astype(int),
    "MI_Rank": mi_scores.rank(ascending=False).astype(int),
    "IG_Rank": ig_scores.rank(ascending=False).astype(int)
})

# Total number of features for Borda Count
num_features = len(ranked_features_df)

# Convert ranks to Borda scores (higher is better)
borda_scores = pd.DataFrame({
    "Feature": ranked_features_df["Feature"],
    "Chi2_Score": num_features - ranked_features_df["Chi2_Rank"] + 1,
    "MI_Score": num_features - ranked_features_df["MI_Rank"] + 1,
    "IG_Score": num_features - ranked_features_df["IG_Rank"] + 1
})
borda_scores["Borda_Total"] = borda_scores[["Chi2_Score", "MI_Score", "IG_Score"]].sum(axis=1)

# Sort Borda Count results
balanced_BordaCount_df = borda_scores.sort_values("Borda_Total", ascending=False).reset_index(drop=True)
balanced_BordaCount_df

# Plotting the Borda Count results
plt.figure(figsize=(12, 8))
plt.barh(balanced_BordaCount_df['Feature'], balanced_BordaCount_df['Borda_Total'])
plt.title("Feature Importance on balanced dataset")
plt.xlabel("Borda Score")
plt.ylabel("Features")
plt.gca().invert_yaxis()  # Highest score at the top
plt.tight_layout()
plt.show()

# Create Rank-Score plot data
chi2_ranked = chi2_scaled.sort_values(ascending=False)
mi_ranked = mi_scaled.sort_values(ascending=False)
ig_ranked = ig_scaled.sort_values(ascending=False)

# Prepare data for plotting
plot_data = [
    (range(1, len(chi2_ranked)+1), chi2_ranked.values, 'Chi²'),
    (range(1, len(mi_ranked)+1), mi_ranked.values, 'Mutual Information'),
    (range(1, len(ig_ranked)+1), ig_ranked.values, 'Information Gain')
]

# Plot
plt.figure(figsize=(12, 7))
markers = ['o', 's', '^']
for (ranks, scores, label), marker in zip(plot_data, markers):
    plt.plot(ranks, scores, label=label, marker=marker)

plt.xlabel("Feature Rank")
plt.ylabel("Normalized Score")
plt.title("Rank-Score Graphs on balanced")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

#!pip install catboost

# Define the new feature sets for Top-K
feature_sets = {}
for k in range(2, 17, 2):  # 2, 4, 6, ..., 16
    key = f'Top{k}'
    feature_sets[key] = balanced_BordaCount_df['Feature'].head(k).tolist()

# Define models
models = {
    'LogisticRegression': Pipeline([
        ('scaler', MinMaxScaler()),
        ('model', LogisticRegression(C=1, class_weight='balanced', max_iter=1000))
    ]),
    'RandomForest': RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42),
    'LightGBM': LGBMClassifier(n_estimators=100, is_unbalance =  True, random_state=42),
    'CatBoost': CatBoostClassifier(depth=4, verbose=0, class_weights = [1, 10, 5],  random_state=42),
    'ExtraTrees': ExtraTreesClassifier(n_estimators=100, class_weight='balanced', random_state=42)
}

# Store results
results = []

# Evaluate each model on each feature set
for name, features in feature_sets.items():
    X_train_k = X_train_sm[features]
    X_val_k = X_val1[features]

    for model_name, model in models.items():
        model.fit(X_train_k, y_train_sm)
        preds = model.predict(X_val_k)

        # Calculate metrics
        # acc = accuracy_score(y_val1, preds)
        # prec = precision_score(y_val1, preds, average='weighted', zero_division=0)
        # rec = recall_score(y_val1, preds, average='weighted', zero_division=0)
        # f1 = f1_score(y_val1, preds, average='weighted', zero_division=0)

        # If possible, get prediction probabilities for AUC
        try:
            probs = model.predict_proba(X_val_k)
            auc = roc_auc_score(y_val1, probs, multi_class='ovr', average='weighted')
        except:
            auc = np.nan

        '''results.append({
            'Model': model_name,
            'Feature_Set': name,
            'Accuracy': acc,
            'Precision': prec,
            'Recall': rec,
            'F1_Score': f1,
            'AUC': auc
        })'''
        macro_f1 = f1_score(y_val1, preds, average='macro', zero_division=0)
        recall = recall_score(y_val1, preds, average='macro', zero_division=0)
        results.append({
            'Model': model_name,
            'Feature_Set': name,
            'Recall': recall,
            'Macro_F1': macro_f1,
            'AUC': auc
        })


# Convert to DataFrame
balanced_results_df = pd.DataFrame(results)
balanced_results_df

# Define color palette
palette = sns.color_palette("tab10")

# Plot
plt.figure(figsize=(12, 6))
for i, model in enumerate(balanced_results_df['Model'].unique()):
    subset = balanced_results_df[balanced_results_df['Model'] == model]
    feature_counts = subset['Feature_Set'].str.extract('(\d+)').astype(int)
    plt.plot(
        feature_counts[0],
        subset['Macro_F1'],
        marker='o',
        label=model,
        color=palette[i]
    )

plt.title("Top-k feature with Macro F1 Score")
plt.xlabel("Top-k Features")
plt.ylabel("Macro F1 Score")
plt.legend(title="Model")
plt.grid(True)
plt.tight_layout()
plt.show()

# Set color palette
sns.set_palette("tab10")

# Prepare AUC plot
plt.figure(figsize=(12, 6))

# Convert feature set names like 'Top2', 'Top4' to numeric values for x-axis
balanced_results_df['Num_Features'] = balanced_results_df['Feature_Set'].str.extract('(\d+)').astype(int)

# Plot AUC for each model
for model in balanced_results_df['Model'].unique():
    model_data = balanced_results_df[balanced_results_df['Model'] == model]
    plt.plot(model_data['Num_Features'], model_data['AUC'], marker='o', label=model)

plt.title("Top-k feature with AUC Score")
plt.xlabel("Top-k features")
plt.ylabel("AUC Score")
plt.legend(title="Model")
plt.grid(True)
plt.tight_layout()
plt.show()

# Assuming X_train_final, y_train_final, X_val2, y_val2, and feature_sets["Top10"] are defined
# Define top10
top10 = feature_sets['Top10']

# Combine training data (X_train + X_val1) if not already done
X_train_concat = pd.concat([X_train, X_val1])
y_train_concat = pd.concat([y_train, y_val1])
from imblearn.pipeline import Pipeline
# Apply hybrid sampling
hybrid_sampler = Pipeline([
    ('smote', SMOTE(random_state=42)),
    ('adasyn', ADASYN(random_state=42))
])
X_train_final, y_train_final = hybrid_sampler.fit_resample(X_train_concat, y_train_concat)

# Dictionary to store best Optuna trials
best_trials = {}

# Objective function for Optuna tuning
def objective(trial, model_name):
    if model_name == 'LogisticRegression':
        C = trial.suggest_loguniform('C', 1e-3, 1e2)
        model = Pipeline([
            ('scaler', MinMaxScaler()),
            ('clf', LogisticRegression(C=C, class_weight='balanced', max_iter=1000))
        ])

    elif model_name == 'RandomForest':
        n_estimators = trial.suggest_int('n_estimators', 50, 300)
        max_depth = trial.suggest_int('max_depth', 2, 20)
        model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, class_weight='balanced', random_state=42)

    elif model_name == 'LightGBM':
        num_leaves = trial.suggest_int('num_leaves', 20, 150)
        max_depth = trial.suggest_int('max_depth', 3, 12)
        learning_rate = trial.suggest_float('learning_rate', 1e-3, 0.3, log=True)
        model = LGBMClassifier(num_leaves=num_leaves, max_depth=max_depth, is_unbalance=True,
                               learning_rate=learning_rate, n_estimators=100,
                               random_state=42)

    elif model_name == 'CatBoost':
        depth = trial.suggest_int('depth', 3, 10)
        learning_rate = trial.suggest_float('learning_rate', 1e-3, 0.3, log=True)
        model = CatBoostClassifier(depth=depth, learning_rate=learning_rate, class_weights=[1, 10, 5],
                                   n_estimators=100, verbose=0, random_state=42, task_type='CPU')

    elif model_name == 'ExtraTrees':
        n_estimators = trial.suggest_int('n_estimators', 50, 200)
        max_depth = trial.suggest_int('max_depth', 2, 20)
        model = ExtraTreesClassifier(n_estimators=n_estimators, max_depth=max_depth,
                                     class_weight='balanced', random_state=42)

    model.fit(X_train_final[top10], y_train_final)
    preds = model.predict_proba(X_val2[top10])
    auc = roc_auc_score(y_val2, preds, multi_class='ovr', average='weighted')
    return auc

# Run optimization for all models
for model_name in ['LogisticRegression', 'RandomForest', 'LightGBM', 'CatBoost', 'ExtraTrees']:
    print(f"Optimizing {model_name}...")
    study = optuna.create_study(direction='maximize')
    study.optimize(lambda trial: objective(trial, model_name), n_trials=30, show_progress_bar=True)
    best_trials[model_name] = study.best_trial
    print(f"Best AUC for {model_name}: {study.best_value:.4f}")
    print(f"Best Params: {study.best_params}")

# Display the best hyperparameters and AUC scores
for model_name, trial in best_trials.items():
    print(f"Model: {model_name}")
    print(f"  Best AUC Score : {trial.value:.4f}")
    print(f"  Best Parameters: {trial.params}")
    print("-" * 50)

test_results = []
def train_model(model_name, params):
    if model_name == 'LogisticRegression':
        model = Pipeline([
            ('scaler', MinMaxScaler()),
            ('clf', LogisticRegression(C=params['C'], max_iter=1000, class_weight='balanced', random_state=42))
        ])
    elif model_name == 'RandomForest':
        model = RandomForestClassifier(n_estimators=params['n_estimators'], class_weight='balanced',
                                       max_depth=params['max_depth'], random_state=42)
    elif model_name == 'LightGBM':
        model = LGBMClassifier(num_leaves=params['num_leaves'], max_depth=params['max_depth'], is_unbalance=True,
                               learning_rate=params['learning_rate'], n_estimators=100, random_state=42)
    elif model_name == 'CatBoost':
        model = CatBoostClassifier(depth=params['depth'], learning_rate=params['learning_rate'], class_weights=[1,10,5],
                                   n_estimators=100, verbose=0, random_state=42, task_type='CPU')
    elif model_name == 'ExtraTrees':
        model = ExtraTreesClassifier(n_estimators=params['n_estimators'], max_depth=params['max_depth'],
                                     class_weight='balanced', random_state=42)
    else:
        raise ValueError(f"Unsupported model: {model_name}")

    model.fit(X_train_final[top10], y_train_final)
    return model

def find_best_threshold(probs, y_true, class_index=1):
    class_probs = probs[:, class_index]
    precisions, recalls, thresholds = precision_recall_curve(y_true == class_index, class_probs)
    f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-8)
    best_idx = np.argmax(f1_scores)
    best_threshold = thresholds[best_idx] if best_idx < len(thresholds) else 1.0
    print(f"Best threshold for class {class_index}: {best_threshold:.3f} with F1 score: {f1_scores[best_idx]:.4f}")

    plt.figure(figsize=(8,6))
    plt.plot(thresholds, precisions[:-1], label='Precision')
    plt.plot(thresholds, recalls[:-1], label='Recall')
    plt.plot(thresholds, f1_scores[:-1], label='F1 Score')
    plt.axvline(best_threshold, color='r', linestyle='--', label='Best Threshold')
    plt.xlabel('Threshold')
    plt.ylabel('Score')
    plt.title(f'Precision, Recall, F1 vs Threshold for class {class_index}')
    plt.legend()
    plt.show()

    return best_threshold

def evaluate_model_with_optimal_threshold(model_name, params):
    model = train_model(model_name, params)
    pred_probs = model.predict_proba(X_test[top10])

    # Find best threshold for class 1
    best_thresh = find_best_threshold(pred_probs, y_test, class_index=1)

    # Apply threshold tuning to predictions
    preds_argmax = np.argmax(pred_probs, axis=1)
    preds_class1 = (pred_probs[:, 1] >= best_thresh).astype(int)
    preds = preds_argmax.copy()
    preds[preds_class1 == 1] = 1

    # Compute metrics
    acc = accuracy_score(y_test, preds)
    prec = precision_score(y_test, preds, average='weighted', zero_division=0)
    rec = recall_score(y_test, preds, average='weighted')
    f1 = f1_score(y_test, preds, average='weighted')
    auc = roc_auc_score(y_test, pred_probs, multi_class='ovr', average='weighted')
    cm = confusion_matrix(y_test, preds)
    report = classification_report(y_test, preds)

    print(f"Final evaluation for {model_name} at threshold {best_thresh:.3f}:")
    print(f"  Accuracy: {acc:.4f}")
    print(f"  Precision (weighted): {prec:.4f}")
    print(f"  Recall (weighted): {rec:.4f}")
    print(f"  F1 Score (weighted): {f1:.4f}")
    print(f"  AUC (weighted, OVR): {auc:.4f}")
    print("  Confusion Matrix:")
    print(cm)
    print("  Classification Report:")
    print(report)
    print("-" * 50)

    # Save results
    test_results.append({
        'Model': model_name,
        'Best Threshold': best_thresh,
        'Accuracy': acc,
        'Precision': prec,
        'Recall': rec,
        'F1 Score': f1,
        'AUC': auc
    })

# Run for all models in best_trials
for model_name, trial in best_trials.items():
    evaluate_model_with_optimal_threshold(model_name, trial.params)

# Store best trials of the models

with open('/content/drive/MyDrive/MRP/best_trials_hybridover.pkl', 'wb') as f:
    pickle.dump(best_trials, f)

# Load best_trials from file
with open('/content/drive/MyDrive/MRP/best_trials_hybridover.pkl', 'rb') as f:
    best_trials_t = pickle.load(f)

# Display the best hyperparameters and AUC scores
for model_name, trial in best_trials.items():
    print(f"Model: {model_name}")
    print(f"  Best AUC Score : {trial.value:.4f}")
    print(f"  Best Parameters: {trial.params}")
    print("-" * 50)

import pandas as pd

# Assuming results is a list of dicts collected during evaluation
HYBRIDover_results_df = pd.DataFrame(test_results)

# Optionally, save to CSV for later use
HYBRIDover_results_df.to_csv('/content/drive/MyDrive/MRP/HYBRIDover_evaluation_results.csv', index=False)

HYBRIDover_results_df

"""# With RUS as imbalance technqiue"""

rus = RandomUnderSampler(random_state=42)
X_train_sm, y_train_sm = rus.fit_resample(X_train, y_train)

print("Before RUS:")
print(y_train.value_counts(normalize=True))
print("\nAfter RUS:")
print(pd.Series(y_train_sm).value_counts(normalize=True))

# Chi²
chi2_selector = SelectKBest(score_func=chi2, k='all').fit(X_train_sm, y_train_sm)
chi2_scores = pd.Series(chi2_selector.scores_, index=X_train.columns)


# Mutual Information
mi_selector = SelectKBest(score_func=mutual_info_classif, k='all').fit(X_train_sm, y_train_sm)
mi_scores = pd.Series(mi_selector.scores_, index=X_train.columns)


# Information Gain (Entropy-Based)
ig_model = DecisionTreeClassifier(criterion='entropy', random_state=42)
ig_model.fit(X_train_sm, y_train_sm)
ig_scores = pd.Series(ig_model.feature_importances_, index=X_train.columns)


# Normalize scores
scaler = MinMaxScaler()
chi2_scaled = pd.Series(scaler.fit_transform(chi2_scores.values.reshape(-1, 1)).flatten(), index=X_train.columns)
mi_scaled = pd.Series(scaler.fit_transform(mi_scores.values.reshape(-1, 1)).flatten(), index=X_train.columns)
ig_scaled = pd.Series(scaler.fit_transform(ig_scores.values.reshape(-1, 1)).flatten(), index=X_train.columns)

ranked_features_df = pd.DataFrame({
    "Feature": chi2_scores.index,
    "Chi2_Rank": chi2_scores.rank(ascending=False).astype(int),
    "MI_Rank": mi_scores.rank(ascending=False).astype(int),
    "IG_Rank": ig_scores.rank(ascending=False).astype(int)
})

# Total number of features for Borda Count
num_features = len(ranked_features_df)

# Convert ranks to Borda scores (higher is better)
borda_scores = pd.DataFrame({
    "Feature": ranked_features_df["Feature"],
    "Chi2_Score": num_features - ranked_features_df["Chi2_Rank"] + 1,
    "MI_Score": num_features - ranked_features_df["MI_Rank"] + 1,
    "IG_Score": num_features - ranked_features_df["IG_Rank"] + 1
})
borda_scores["Borda_Total"] = borda_scores[["Chi2_Score", "MI_Score", "IG_Score"]].sum(axis=1)

# Sort Borda Count results
balanced_BordaCount_df = borda_scores.sort_values("Borda_Total", ascending=False).reset_index(drop=True)
balanced_BordaCount_df

# Plotting the Borda Count results
plt.figure(figsize=(12, 8))
plt.barh(balanced_BordaCount_df['Feature'], balanced_BordaCount_df['Borda_Total'])
plt.title("Feature Importance on balanced dataset")
plt.xlabel("Borda Score")
plt.ylabel("Features")
plt.gca().invert_yaxis()  # Highest score at the top
plt.tight_layout()
plt.show()

# Create Rank-Score plot data
chi2_ranked = chi2_scaled.sort_values(ascending=False)
mi_ranked = mi_scaled.sort_values(ascending=False)
ig_ranked = ig_scaled.sort_values(ascending=False)

# Prepare data for plotting
plot_data = [
    (range(1, len(chi2_ranked)+1), chi2_ranked.values, 'Chi²'),
    (range(1, len(mi_ranked)+1), mi_ranked.values, 'Mutual Information'),
    (range(1, len(ig_ranked)+1), ig_ranked.values, 'Information Gain')
]

# Plot
plt.figure(figsize=(12, 7))
markers = ['o', 's', '^']
for (ranks, scores, label), marker in zip(plot_data, markers):
    plt.plot(ranks, scores, label=label, marker=marker)

plt.xlabel("Feature Rank")
plt.ylabel("Normalized Score")
plt.title("Rank-Score Graphs on balanced")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

#!pip install catboost

# Define the new feature sets for Top-K
feature_sets = {}
for k in range(2, 17, 2):  # 2, 4, 6, ..., 16
    key = f'Top{k}'
    feature_sets[key] = balanced_BordaCount_df['Feature'].head(k).tolist()

# Define models
models = {
    'LogisticRegression': Pipeline([
        ('scaler', MinMaxScaler()),
        ('model', LogisticRegression(C=1, class_weight='balanced', max_iter=1000))
    ]),
    'RandomForest': RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42),
    'LightGBM': LGBMClassifier(n_estimators=100, is_unbalance =  True, random_state=42),
    'CatBoost': CatBoostClassifier(depth=4, verbose=0, class_weights = [1, 10, 5],  random_state=42),
    'ExtraTrees': ExtraTreesClassifier(n_estimators=100, class_weight='balanced', random_state=42)
}

# Store results
results = []

# Evaluate each model on each feature set
for name, features in feature_sets.items():
    X_train_k = X_train_sm[features]
    X_val_k = X_val1[features]

    for model_name, model in models.items():
        model.fit(X_train_k, y_train_sm)
        preds = model.predict(X_val_k)

        # Calculate metrics
        # acc = accuracy_score(y_val1, preds)
        # prec = precision_score(y_val1, preds, average='weighted', zero_division=0)
        # rec = recall_score(y_val1, preds, average='weighted', zero_division=0)
        # f1 = f1_score(y_val1, preds, average='weighted', zero_division=0)

        # If possible, get prediction probabilities for AUC
        try:
            probs = model.predict_proba(X_val_k)
            auc = roc_auc_score(y_val1, probs, multi_class='ovr', average='weighted')
        except:
            auc = np.nan

        '''results.append({
            'Model': model_name,
            'Feature_Set': name,
            'Accuracy': acc,
            'Precision': prec,
            'Recall': rec,
            'F1_Score': f1,
            'AUC': auc
        })'''
        macro_f1 = f1_score(y_val1, preds, average='macro', zero_division=0)
        recall = recall_score(y_val1, preds, average='macro', zero_division=0)
        results.append({
            'Model': model_name,
            'Feature_Set': name,
            'Recall': recall,
            'Macro_F1': macro_f1,
            'AUC': auc
        })


# Convert to DataFrame
balanced_results_df = pd.DataFrame(results)
balanced_results_df

# Define color palette
palette = sns.color_palette("tab10")

# Plot
plt.figure(figsize=(12, 6))
for i, model in enumerate(balanced_results_df['Model'].unique()):
    subset = balanced_results_df[balanced_results_df['Model'] == model]
    feature_counts = subset['Feature_Set'].str.extract('(\d+)').astype(int)
    plt.plot(
        feature_counts[0],
        subset['Macro_F1'],
        marker='o',
        label=model,
        color=palette[i]
    )

plt.title("Top-k feature with Macro F1 Score")
plt.xlabel("Top-k Features")
plt.ylabel("Macro F1 Score")
plt.legend(title="Model")
plt.grid(True)
plt.tight_layout()
plt.show()

# Set color palette
sns.set_palette("tab10")

# Prepare AUC plot
plt.figure(figsize=(12, 6))

# Convert feature set names like 'Top2', 'Top4' to numeric values for x-axis
balanced_results_df['Num_Features'] = balanced_results_df['Feature_Set'].str.extract('(\d+)').astype(int)

# Plot AUC for each model
for model in balanced_results_df['Model'].unique():
    model_data = balanced_results_df[balanced_results_df['Model'] == model]
    plt.plot(model_data['Num_Features'], model_data['AUC'], marker='o', label=model)

plt.title("Top-k feature with AUC Score")
plt.xlabel("Top-k features")
plt.ylabel("AUC Score")
plt.legend(title="Model")
plt.grid(True)
plt.tight_layout()
plt.show()

# Highlight top 10 features
colors = ['tab:blue' if i >= 10 else 'tab:orange' for i in range(len(balanced_BordaCount_df))]

plt.figure(figsize=(12, 8))
plt.barh(balanced_BordaCount_df['Feature'], balanced_BordaCount_df['Borda_Total'], color=colors)
plt.title("Feature Importance")
plt.xlabel("Borda Count (Combined Rank)")
plt.ylabel("Features")
plt.gca().invert_yaxis()  # Highest score at the top
plt.tight_layout()
plt.show()

# Assuming X_train_final, y_train_final, X_val2, y_val2, and feature_sets["Top10"] are defined
# Define top10
top10 = feature_sets['Top10']

# Combine training data (X_train + X_val1) if not already done
X_train_concat = pd.concat([X_train, X_val1])
y_train_concat = pd.concat([y_train, y_val1])

# Apply RUS
rus = RandomUnderSampler(random_state=42)
X_train_final, y_train_final = rus.fit_resample(X_train_concat, y_train_concat)

# Dictionary to store best Optuna trials
best_trials = {}

# Objective function for Optuna tuning
def objective(trial, model_name):
    if model_name == 'LogisticRegression':
        C = trial.suggest_loguniform('C', 1e-3, 1e2)
        model = Pipeline([
            ('scaler', MinMaxScaler()),
            ('clf', LogisticRegression(C=C, class_weight='balanced', max_iter=1000))
        ])

    elif model_name == 'RandomForest':
        n_estimators = trial.suggest_int('n_estimators', 50, 300)
        max_depth = trial.suggest_int('max_depth', 2, 20)
        model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, class_weight='balanced', random_state=42)

    elif model_name == 'LightGBM':
        num_leaves = trial.suggest_int('num_leaves', 20, 150)
        max_depth = trial.suggest_int('max_depth', 3, 12)
        learning_rate = trial.suggest_float('learning_rate', 1e-3, 0.3, log=True)
        model = LGBMClassifier(num_leaves=num_leaves, max_depth=max_depth, is_unbalance=True,
                               learning_rate=learning_rate, n_estimators=100,
                               random_state=42)

    elif model_name == 'CatBoost':
        depth = trial.suggest_int('depth', 3, 10)
        learning_rate = trial.suggest_float('learning_rate', 1e-3, 0.3, log=True)
        model = CatBoostClassifier(depth=depth, learning_rate=learning_rate, class_weights=[1, 10, 5],
                                   n_estimators=100, verbose=0, random_state=42, task_type='CPU')

    elif model_name == 'ExtraTrees':
        n_estimators = trial.suggest_int('n_estimators', 50, 200)
        max_depth = trial.suggest_int('max_depth', 2, 20)
        model = ExtraTreesClassifier(n_estimators=n_estimators, max_depth=max_depth,
                                     class_weight='balanced', random_state=42)

    model.fit(X_train_final[top10], y_train_final)
    preds = model.predict_proba(X_val2[top10])
    auc = roc_auc_score(y_val2, preds, multi_class='ovr', average='weighted')
    return auc

# Run optimization for all models
for model_name in ['LogisticRegression', 'RandomForest', 'LightGBM', 'CatBoost', 'ExtraTrees']:
    print(f"Optimizing {model_name}...")
    study = optuna.create_study(direction='maximize')
    study.optimize(lambda trial: objective(trial, model_name), n_trials=30, show_progress_bar=True)
    best_trials[model_name] = study.best_trial
    print(f"Best AUC for {model_name}: {study.best_value:.4f}")
    print(f"Best Params: {study.best_params}")

# Display the best hyperparameters and AUC scores
for model_name, trial in best_trials.items():
    print(f"Model: {model_name}")
    print(f"  Best AUC Score : {trial.value:.4f}")
    print(f"  Best Parameters: {trial.params}")
    print("-" * 50)

test_results = []
def train_model(model_name, params):
    if model_name == 'LogisticRegression':
        model = Pipeline([
            ('scaler', MinMaxScaler()),
            ('clf', LogisticRegression(C=params['C'], max_iter=1000, class_weight='balanced', random_state=42))
        ])
    elif model_name == 'RandomForest':
        model = RandomForestClassifier(n_estimators=params['n_estimators'], class_weight='balanced',
                                       max_depth=params['max_depth'], random_state=42)
    elif model_name == 'LightGBM':
        model = LGBMClassifier(num_leaves=params['num_leaves'], max_depth=params['max_depth'], is_unbalance=True,
                               learning_rate=params['learning_rate'], n_estimators=100, random_state=42)
    elif model_name == 'CatBoost':
        model = CatBoostClassifier(depth=params['depth'], learning_rate=params['learning_rate'], class_weights=[1,10,5],
                                   n_estimators=100, verbose=0, random_state=42, task_type='CPU')
    elif model_name == 'ExtraTrees':
        model = ExtraTreesClassifier(n_estimators=params['n_estimators'], max_depth=params['max_depth'],
                                     class_weight='balanced', random_state=42)
    else:
        raise ValueError(f"Unsupported model: {model_name}")

    model.fit(X_train_final[top10], y_train_final)
    return model

def find_best_threshold(probs, y_true, class_index=1):
    class_probs = probs[:, class_index]
    precisions, recalls, thresholds = precision_recall_curve(y_true == class_index, class_probs)
    f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-8)
    best_idx = np.argmax(f1_scores)
    best_threshold = thresholds[best_idx] if best_idx < len(thresholds) else 1.0
    print(f"Best threshold for class {class_index}: {best_threshold:.3f} with F1 score: {f1_scores[best_idx]:.4f}")

    plt.figure(figsize=(8,6))
    plt.plot(thresholds, precisions[:-1], label='Precision')
    plt.plot(thresholds, recalls[:-1], label='Recall')
    plt.plot(thresholds, f1_scores[:-1], label='F1 Score')
    plt.axvline(best_threshold, color='r', linestyle='--', label='Best Threshold')
    plt.xlabel('Threshold')
    plt.ylabel('Score')
    plt.title(f'Precision, Recall, F1 vs Threshold for class {class_index}')
    plt.legend()
    plt.show()

    return best_threshold

def evaluate_model_with_optimal_threshold(model_name, params):
    model = train_model(model_name, params)
    pred_probs = model.predict_proba(X_test[top10])

    # Find best threshold for class 1
    best_thresh = find_best_threshold(pred_probs, y_test, class_index=1)

    # Apply threshold tuning to predictions
    preds_argmax = np.argmax(pred_probs, axis=1)
    preds_class1 = (pred_probs[:, 1] >= best_thresh).astype(int)
    preds = preds_argmax.copy()
    preds[preds_class1 == 1] = 1

    # Compute metrics
    acc = accuracy_score(y_test, preds)
    prec = precision_score(y_test, preds, average='weighted', zero_division=0)
    rec = recall_score(y_test, preds, average='weighted')
    f1 = f1_score(y_test, preds, average='weighted')
    auc = roc_auc_score(y_test, pred_probs, multi_class='ovr', average='weighted')
    cm = confusion_matrix(y_test, preds)
    report = classification_report(y_test, preds)

    print(f"Final evaluation for {model_name} at threshold {best_thresh:.3f}:")
    print(f"  Accuracy: {acc:.4f}")
    print(f"  Precision (weighted): {prec:.4f}")
    print(f"  Recall (weighted): {rec:.4f}")
    print(f"  F1 Score (weighted): {f1:.4f}")
    print(f"  AUC (weighted, OVR): {auc:.4f}")
    print("  Confusion Matrix:")
    print(cm)
    print("  Classification Report:")
    print(report)
    print("-" * 50)

    # Save results
    test_results.append({
        'Model': model_name,
        'Best Threshold': best_thresh,
        'Accuracy': acc,
        'Precision': prec,
        'Recall': rec,
        'F1 Score': f1,
        'AUC': auc
    })

# Run for all models in best_trials
for model_name, trial in best_trials.items():
    evaluate_model_with_optimal_threshold(model_name, trial.params)

# Store best trials of the model

with open('/content/drive/MyDrive/MRP/best_trials_RUS.pkl', 'wb') as f:
    pickle.dump(best_trials, f)

# Load best_trials from file
#with open('best_trials.pkl', 'rb') as f:
#    best_trials_t = pickle.load(f)

# Assuming results is a list of dicts collected during evaluation
RUS_results_df = pd.DataFrame(test_results)

# Optionally, save to CSV for later use
RUS_results_df.to_csv('/content/drive/MyDrive/MRP/RUS_evaluation_results.csv', index=False)

"""# With Tomek-links as imbalance technique"""

tl = TomekLinks()
X_train_sm, y_train_sm = tl.fit_resample(X_train, y_train)

# Step 2 A: Check class balance after resampling
print("Before Tomek:")
print(y_train.value_counts(normalize=True))
print("\nAfter Tomek:")
print(pd.Series(y_train_sm).value_counts(normalize=True))

len(X_train_sm)

# Chi²
chi2_selector = SelectKBest(score_func=chi2, k='all').fit(X_train_sm, y_train_sm)
chi2_scores = pd.Series(chi2_selector.scores_, index=X_train.columns)


# Mutual Information
mi_selector = SelectKBest(score_func=mutual_info_classif, k='all').fit(X_train_sm, y_train_sm)
mi_scores = pd.Series(mi_selector.scores_, index=X_train.columns)


# Information Gain (Entropy-Based)
ig_model = DecisionTreeClassifier(criterion='entropy', random_state=42)
ig_model.fit(X_train_sm, y_train_sm)
ig_scores = pd.Series(ig_model.feature_importances_, index=X_train.columns)


# Normalize scores
scaler = MinMaxScaler()
chi2_scaled = pd.Series(scaler.fit_transform(chi2_scores.values.reshape(-1, 1)).flatten(), index=X_train.columns)
mi_scaled = pd.Series(scaler.fit_transform(mi_scores.values.reshape(-1, 1)).flatten(), index=X_train.columns)
ig_scaled = pd.Series(scaler.fit_transform(ig_scores.values.reshape(-1, 1)).flatten(), index=X_train.columns)

# Create rank-based DataFrame without AvgRank
ranked_features_df = pd.DataFrame({
    "Feature": chi2_scores.index,
    "Chi2_Rank": chi2_scores.rank(ascending=False).astype(int),
    "MI_Rank": mi_scores.rank(ascending=False).astype(int),
    "IG_Rank": ig_scores.rank(ascending=False).astype(int)
})

# Total number of features for Borda Count
num_features = len(ranked_features_df)

# Convert ranks to Borda scores (higher is better)
borda_scores = pd.DataFrame({
    "Feature": ranked_features_df["Feature"],
    "Chi2_Score": num_features - ranked_features_df["Chi2_Rank"] + 1,
    "MI_Score": num_features - ranked_features_df["MI_Rank"] + 1,
    "IG_Score": num_features - ranked_features_df["IG_Rank"] + 1
})
borda_scores["Borda_Total"] = borda_scores[["Chi2_Score", "MI_Score", "IG_Score"]].sum(axis=1)

# Sort Borda Count results
balanced_BordaCount_df = borda_scores.sort_values("Borda_Total", ascending=False).reset_index(drop=True)
balanced_BordaCount_df

# Plotting the Borda Count results
plt.figure(figsize=(12, 8))
plt.barh(balanced_BordaCount_df['Feature'], balanced_BordaCount_df['Borda_Total'])
plt.title("Feature Importance on balanced dataset")
plt.xlabel("Borda Score")
plt.ylabel("Features")
plt.gca().invert_yaxis()  # Highest score at the top
plt.tight_layout()
plt.show()

# Create Rank-Score plot data
chi2_ranked = chi2_scaled.sort_values(ascending=False)
mi_ranked = mi_scaled.sort_values(ascending=False)
ig_ranked = ig_scaled.sort_values(ascending=False)

# Prepare data for plotting
plot_data = [
    (range(1, len(chi2_ranked)+1), chi2_ranked.values, 'Chi²'),
    (range(1, len(mi_ranked)+1), mi_ranked.values, 'Mutual Information'),
    (range(1, len(ig_ranked)+1), ig_ranked.values, 'Information Gain')
]

# Plot
plt.figure(figsize=(12, 7))
markers = ['o', 's', '^']
for (ranks, scores, label), marker in zip(plot_data, markers):
    plt.plot(ranks, scores, label=label, marker=marker)

plt.xlabel("Feature Rank")
plt.ylabel("Normalized Score")
plt.title("Rank-Score Graphs on balanced")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

#!pip install catboost

# Define the new feature sets for Top-K
feature_sets = {}
for k in range(2, 17, 2):  # 2, 4, 6, ..., 16
    key = f'Top{k}'
    feature_sets[key] = balanced_BordaCount_df['Feature'].head(k).tolist()

# Define models
models = {
    'LogisticRegression': Pipeline([
        ('scaler', MinMaxScaler()),
        ('model', LogisticRegression(C=1, class_weight='balanced', max_iter=1000))
    ]),
    'RandomForest': RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42),
    'LightGBM': LGBMClassifier(n_estimators=100, is_unbalance =  True, random_state=42),
    'CatBoost': CatBoostClassifier(depth=4, verbose=0, class_weights = [1, 10, 5],  random_state=42),
    'ExtraTrees': ExtraTreesClassifier(n_estimators=100, class_weight='balanced', random_state=42)
}

# Store results
results = []

# Evaluate each model on each feature set
for name, features in feature_sets.items():
    X_train_k = X_train_sm[features]
    X_val_k = X_val1[features]

    for model_name, model in models.items():
        model.fit(X_train_k, y_train_sm)
        preds = model.predict(X_val_k)

        # Calculate metrics
        # acc = accuracy_score(y_val1, preds)
        # prec = precision_score(y_val1, preds, average='weighted', zero_division=0)
        # rec = recall_score(y_val1, preds, average='weighted', zero_division=0)
        # f1 = f1_score(y_val1, preds, average='weighted', zero_division=0)

        # If possible, get prediction probabilities for AUC
        try:
            probs = model.predict_proba(X_val_k)
            auc = roc_auc_score(y_val1, probs, multi_class='ovr', average='weighted')
        except:
            auc = np.nan

        '''results.append({
            'Model': model_name,
            'Feature_Set': name,
            'Accuracy': acc,
            'Precision': prec,
            'Recall': rec,
            'F1_Score': f1,
            'AUC': auc
        })'''
        macro_f1 = f1_score(y_val1, preds, average='macro', zero_division=0)
        recall = recall_score(y_val1, preds, average='macro', zero_division=0)
        results.append({
            'Model': model_name,
            'Feature_Set': name,
            'Recall': recall,
            'Macro_F1': macro_f1,
            'AUC': auc
        })


# Convert to DataFrame
balanced_results_df = pd.DataFrame(results)
balanced_results_df

# Define color palette
palette = sns.color_palette("tab10")

# Plot
plt.figure(figsize=(12, 6))
for i, model in enumerate(balanced_results_df['Model'].unique()):
    subset = balanced_results_df[balanced_results_df['Model'] == model]
    feature_counts = subset['Feature_Set'].str.extract('(\d+)').astype(int)
    plt.plot(
        feature_counts[0],
        subset['Macro_F1'],
        marker='o',
        label=model,
        color=palette[i]
    )

plt.title("Top-k feature with Macro F1 Score")
plt.xlabel("Top-k Features")
plt.ylabel("Macro F1 Score")
plt.legend(title="Model")
plt.grid(True)
plt.tight_layout()
plt.show()

# Set color palette
sns.set_palette("tab10")

# Prepare AUC plot
plt.figure(figsize=(12, 6))

# Convert feature set names like 'Top2', 'Top4' to numeric values for x-axis
balanced_results_df['Num_Features'] = balanced_results_df['Feature_Set'].str.extract('(\d+)').astype(int)

# Plot AUC for each model
for model in balanced_results_df['Model'].unique():
    model_data = balanced_results_df[balanced_results_df['Model'] == model]
    plt.plot(model_data['Num_Features'], model_data['AUC'], marker='o', label=model)

plt.title("Top-k feature with AUC Score")
plt.xlabel("Top-k features")
plt.ylabel("AUC Score")
plt.legend(title="Model")
plt.grid(True)
plt.tight_layout()
plt.show()

# Highlight top 10 features
colors = ['tab:blue' if i >= 10 else 'tab:orange' for i in range(len(balanced_BordaCount_df))]

plt.figure(figsize=(12, 8))
plt.barh(balanced_BordaCount_df['Feature'], balanced_BordaCount_df['Borda_Total'], color=colors)
plt.title("Feature Importance")
plt.xlabel("Borda Count (Combined Rank)")
plt.ylabel("Features")
plt.gca().invert_yaxis()  # Highest score at the top
plt.tight_layout()
plt.show()

import optuna
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier
from sklearn.metrics import roc_auc_score

# Assuming X_train_final, y_train_final, X_val2, y_val2, and feature_sets["Top10"] are defined
# Define top10
top10 = feature_sets['Top10']

# Combine training data (X_train + X_val1) if not already done
X_train_concat = pd.concat([X_train, X_val1])
y_train_concat = pd.concat([y_train, y_val1])

# Apply Tomek-links
t = TomekLinks()
X_train_final, y_train_final = t.fit_resample(X_train_concat[top10], y_train_concat)

# Dictionary to store best Optuna trials
best_trials = {}

# Objective function for Optuna tuning
def objective(trial, model_name):
    if model_name == 'LogisticRegression':
        C = trial.suggest_loguniform('C', 1e-3, 1e2)
        model = Pipeline([
            ('scaler', MinMaxScaler()),
            ('clf', LogisticRegression(C=C, class_weight='balanced', max_iter=1000))
        ])

    elif model_name == 'RandomForest':
        n_estimators = trial.suggest_int('n_estimators', 50, 300)
        max_depth = trial.suggest_int('max_depth', 2, 20)
        model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, class_weight='balanced', random_state=42)

    elif model_name == 'LightGBM':
        num_leaves = trial.suggest_int('num_leaves', 20, 150)
        max_depth = trial.suggest_int('max_depth', 3, 12)
        learning_rate = trial.suggest_float('learning_rate', 1e-3, 0.3, log=True)
        model = LGBMClassifier(num_leaves=num_leaves, max_depth=max_depth, is_unbalance=True,
                               learning_rate=learning_rate, n_estimators=100,
                               random_state=42)

    elif model_name == 'CatBoost':
        depth = trial.suggest_int('depth', 3, 10)
        learning_rate = trial.suggest_float('learning_rate', 1e-3, 0.3, log=True)
        model = CatBoostClassifier(depth=depth, learning_rate=learning_rate, class_weights=[1, 10, 5],
                                   n_estimators=100, verbose=0, random_state=42, task_type='CPU')

    elif model_name == 'ExtraTrees':
        n_estimators = trial.suggest_int('n_estimators', 50, 200)
        max_depth = trial.suggest_int('max_depth', 2, 20)
        model = ExtraTreesClassifier(n_estimators=n_estimators, max_depth=max_depth,
                                     class_weight='balanced', random_state=42)

    model.fit(X_train_final[top10], y_train_final)
    preds = model.predict_proba(X_val2[top10])
    auc = roc_auc_score(y_val2, preds, multi_class='ovr', average='weighted')
    return auc

# Run optimization for all models
for model_name in ['LogisticRegression', 'RandomForest', 'LightGBM', 'CatBoost', 'ExtraTrees']:
    print(f"Optimizing {model_name}...")
    study = optuna.create_study(direction='maximize')
    study.optimize(lambda trial: objective(trial, model_name), n_trials=30, show_progress_bar=True)
    best_trials[model_name] = study.best_trial
    print(f"Best AUC for {model_name}: {study.best_value:.4f}")
    print(f"Best Params: {study.best_params}")

# Display the best hyperparameters and AUC scores
for model_name, trial in best_trials.items():
    print(f"Model: {model_name}")
    print(f"  Best AUC Score : {trial.value:.4f}")
    print(f"  Best Parameters: {trial.params}")
    print("-" * 50)

test_results = []
def train_model(model_name, params):
    if model_name == 'LogisticRegression':
        model = Pipeline([
            ('scaler', MinMaxScaler()),
            ('clf', LogisticRegression(C=params['C'], max_iter=1000, class_weight='balanced', random_state=42))
        ])
    elif model_name == 'RandomForest':
        model = RandomForestClassifier(n_estimators=params['n_estimators'], class_weight='balanced',
                                       max_depth=params['max_depth'], random_state=42)
    elif model_name == 'LightGBM':
        model = LGBMClassifier(num_leaves=params['num_leaves'], max_depth=params['max_depth'], is_unbalance=True,
                               learning_rate=params['learning_rate'], n_estimators=100, random_state=42)
    elif model_name == 'CatBoost':
        model = CatBoostClassifier(depth=params['depth'], learning_rate=params['learning_rate'], class_weights=[1,10,5],
                                   n_estimators=100, verbose=0, random_state=42, task_type='CPU')
    elif model_name == 'ExtraTrees':
        model = ExtraTreesClassifier(n_estimators=params['n_estimators'], max_depth=params['max_depth'],
                                     class_weight='balanced', random_state=42)
    else:
        raise ValueError(f"Unsupported model: {model_name}")

    model.fit(X_train_final[top10], y_train_final)
    return model

def find_best_threshold(probs, y_true, class_index=1):
    class_probs = probs[:, class_index]
    precisions, recalls, thresholds = precision_recall_curve(y_true == class_index, class_probs)
    f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-8)
    best_idx = np.argmax(f1_scores)
    best_threshold = thresholds[best_idx] if best_idx < len(thresholds) else 1.0
    print(f"Best threshold for class {class_index}: {best_threshold:.3f} with F1 score: {f1_scores[best_idx]:.4f}")

    plt.figure(figsize=(8,6))
    plt.plot(thresholds, precisions[:-1], label='Precision')
    plt.plot(thresholds, recalls[:-1], label='Recall')
    plt.plot(thresholds, f1_scores[:-1], label='F1 Score')
    plt.axvline(best_threshold, color='r', linestyle='--', label='Best Threshold')
    plt.xlabel('Threshold')
    plt.ylabel('Score')
    plt.title(f'Precision, Recall, F1 vs Threshold for class {class_index}')
    plt.legend()
    plt.show()

    return best_threshold

def evaluate_model_with_optimal_threshold(model_name, params):
    model = train_model(model_name, params)
    pred_probs = model.predict_proba(X_test[top10])

    # Find best threshold for class 1
    best_thresh = find_best_threshold(pred_probs, y_test, class_index=1)

    # Apply threshold tuning to predictions
    preds_argmax = np.argmax(pred_probs, axis=1)
    preds_class1 = (pred_probs[:, 1] >= best_thresh).astype(int)
    preds = preds_argmax.copy()
    preds[preds_class1 == 1] = 1

    # Compute metrics
    acc = accuracy_score(y_test, preds)
    prec = precision_score(y_test, preds, average='weighted', zero_division=0)
    rec = recall_score(y_test, preds, average='weighted')
    f1 = f1_score(y_test, preds, average='weighted')
    auc = roc_auc_score(y_test, pred_probs, multi_class='ovr', average='weighted')
    cm = confusion_matrix(y_test, preds)
    report = classification_report(y_test, preds)

    print(f"Final evaluation for {model_name} at threshold {best_thresh:.3f}:")
    print(f"  Accuracy: {acc:.4f}")
    print(f"  Precision (weighted): {prec:.4f}")
    print(f"  Recall (weighted): {rec:.4f}")
    print(f"  F1 Score (weighted): {f1:.4f}")
    print(f"  AUC (weighted, OVR): {auc:.4f}")
    print("  Confusion Matrix:")
    print(cm)
    print("  Classification Report:")
    print(report)
    print("-" * 50)

    # Save results
    test_results.append({
        'Model': model_name,
        'Best Threshold': best_thresh,
        'Accuracy': acc,
        'Precision': prec,
        'Recall': rec,
        'F1 Score': f1,
        'AUC': auc
    })

# Run for all models in best_trials
for model_name, trial in best_trials.items():
    evaluate_model_with_optimal_threshold(model_name, trial.params)

# Store best trials of the model
with open('/content/drive/MyDrive/MRP/best_trials_Tomek.pkl', 'wb') as f:
    pickle.dump(best_trials, f)

# Load best_trials from file
#with open('best_trials_Tomek.pkl', 'rb') as f:
#    best_trials_t = pickle.load(f)

# Assuming results is a list of dicts collected during evaluation
Tomek_results_df = pd.DataFrame(test_results)

# Optionally, save to CSV for later use
Tomek_results_df.to_csv('/content/drive/MyDrive/MRP/Tomek_evaluation_results.csv', index=False)

"""# With Hybrid undersample"""

pipeline = Pipeline([
    ('rus', RandomUnderSampler(random_state=42)),
    ('tl', TomekLinks())
])

X_train_sm, y_train_sm = pipeline.fit_resample(X_train, y_train)

print("Before Hybrid Undersample:")
print(y_train.value_counts(normalize=True))
print("\nAfter Hybrid Undersample:")
print(pd.Series(y_train_sm).value_counts(normalize=True))

len(X_train_sm)

# Chi²
chi2_selector = SelectKBest(score_func=chi2, k='all').fit(X_train_sm, y_train_sm)
chi2_scores = pd.Series(chi2_selector.scores_, index=X_train.columns)


# Mutual Information
mi_selector = SelectKBest(score_func=mutual_info_classif, k='all').fit(X_train_sm, y_train_sm)
mi_scores = pd.Series(mi_selector.scores_, index=X_train.columns)


# Information Gain (Entropy-Based)
ig_model = DecisionTreeClassifier(criterion='entropy', random_state=42)
ig_model.fit(X_train_sm, y_train_sm)
ig_scores = pd.Series(ig_model.feature_importances_, index=X_train.columns)


# Normalize scores
scaler = MinMaxScaler()
chi2_scaled = pd.Series(scaler.fit_transform(chi2_scores.values.reshape(-1, 1)).flatten(), index=X_train.columns)
mi_scaled = pd.Series(scaler.fit_transform(mi_scores.values.reshape(-1, 1)).flatten(), index=X_train.columns)
ig_scaled = pd.Series(scaler.fit_transform(ig_scores.values.reshape(-1, 1)).flatten(), index=X_train.columns)

# Combine scores (equal weight)
#combined_score = chi2_scaled + mi_scaled + ig_scaled
#top_features = combined_score.sort_values(ascending=False).head(20)

# Create summary DataFrame
#top_features_df = pd.DataFrame({
#    "Feature": top_features.index,
#    "CombinedScore": top_features.values,
#    "Chi2_Score": chi2_scores[top_features.index].values,
#    "MI_Score": mi_scores[top_features.index].values,
#    "IG_Score": ig_scores[top_features.index].values
#})

ranked_features_df = pd.DataFrame({
    "Feature": chi2_scores.index,
    "Chi2_Rank": chi2_scores.rank(ascending=False).astype(int),
    "MI_Rank": mi_scores.rank(ascending=False).astype(int),
    "IG_Rank": ig_scores.rank(ascending=False).astype(int)
})

# Total number of features for Borda Count
num_features = len(ranked_features_df)

# Convert ranks to Borda scores (higher is better)
borda_scores = pd.DataFrame({
    "Feature": ranked_features_df["Feature"],
    "Chi2_Score": num_features - ranked_features_df["Chi2_Rank"] + 1,
    "MI_Score": num_features - ranked_features_df["MI_Rank"] + 1,
    "IG_Score": num_features - ranked_features_df["IG_Rank"] + 1
})
borda_scores["Borda_Total"] = borda_scores[["Chi2_Score", "MI_Score", "IG_Score"]].sum(axis=1)

# Sort Borda Count results
balanced_BordaCount_df = borda_scores.sort_values("Borda_Total", ascending=False).reset_index(drop=True)
balanced_BordaCount_df

# Plotting the Borda Count results
plt.figure(figsize=(12, 8))
plt.barh(balanced_BordaCount_df['Feature'], balanced_BordaCount_df['Borda_Total'])
plt.title("Feature Importance on balanced dataset")
plt.xlabel("Borda Score")
plt.ylabel("Features")
plt.gca().invert_yaxis()  # Highest score at the top
plt.tight_layout()
plt.show()

# Create Rank-Score plot data
chi2_ranked = chi2_scaled.sort_values(ascending=False)
mi_ranked = mi_scaled.sort_values(ascending=False)
ig_ranked = ig_scaled.sort_values(ascending=False)

# Prepare data for plotting
plot_data = [
    (range(1, len(chi2_ranked)+1), chi2_ranked.values, 'Chi²'),
    (range(1, len(mi_ranked)+1), mi_ranked.values, 'Mutual Information'),
    (range(1, len(ig_ranked)+1), ig_ranked.values, 'Information Gain')
]

# Plot
plt.figure(figsize=(12, 7))
markers = ['o', 's', '^']
for (ranks, scores, label), marker in zip(plot_data, markers):
    plt.plot(ranks, scores, label=label, marker=marker)

plt.xlabel("Feature Rank")
plt.ylabel("Normalized Score")
plt.title("Rank-Score Graphs on balanced")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

#!pip install catboost

# Define the new feature sets for Top-K
feature_sets = {}
for k in range(2, 17, 2):  # 2, 4, 6, ..., 16
    key = f'Top{k}'
    feature_sets[key] = balanced_BordaCount_df['Feature'].head(k).tolist()

# Define models
models = {
    'LogisticRegression': Pipeline([
        ('scaler', MinMaxScaler()),
        ('model', LogisticRegression(C=1, class_weight='balanced', max_iter=1000))
    ]),
    'RandomForest': RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42),
    'LightGBM': LGBMClassifier(n_estimators=100, is_unbalance =  True, random_state=42),
    'CatBoost': CatBoostClassifier(depth=4, verbose=0, class_weights = [1, 10, 5],  random_state=42),
    'ExtraTrees': ExtraTreesClassifier(n_estimators=100, class_weight='balanced', random_state=42)
}

# Store results
results = []

# Evaluate each model on each feature set
for name, features in feature_sets.items():
    X_train_k = X_train_sm[features]
    X_val_k = X_val1[features]

    for model_name, model in models.items():
        model.fit(X_train_k, y_train_sm)
        preds = model.predict(X_val_k)

        # Calculate metrics
        # acc = accuracy_score(y_val1, preds)
        # prec = precision_score(y_val1, preds, average='weighted', zero_division=0)
        # rec = recall_score(y_val1, preds, average='weighted', zero_division=0)
        # f1 = f1_score(y_val1, preds, average='weighted', zero_division=0)

        # If possible, get prediction probabilities for AUC
        try:
            probs = model.predict_proba(X_val_k)
            auc = roc_auc_score(y_val1, probs, multi_class='ovr', average='weighted')
        except:
            auc = np.nan

        '''results.append({
            'Model': model_name,
            'Feature_Set': name,
            'Accuracy': acc,
            'Precision': prec,
            'Recall': rec,
            'F1_Score': f1,
            'AUC': auc
        })'''
        macro_f1 = f1_score(y_val1, preds, average='macro', zero_division=0)
        recall = recall_score(y_val1, preds, average='macro', zero_division=0)
        results.append({
            'Model': model_name,
            'Feature_Set': name,
            'Recall': recall,
            'Macro_F1': macro_f1,
            'AUC': auc
        })


# Convert to DataFrame
balanced_results_df = pd.DataFrame(results)
balanced_results_df

# Define color palette
palette = sns.color_palette("tab10")

# Plot
plt.figure(figsize=(12, 6))
for i, model in enumerate(balanced_results_df['Model'].unique()):
    subset = balanced_results_df[balanced_results_df['Model'] == model]
    feature_counts = subset['Feature_Set'].str.extract('(\d+)').astype(int)
    plt.plot(
        feature_counts[0],
        subset['Macro_F1'],
        marker='o',
        label=model,
        color=palette[i]
    )

plt.title("Top-k feature with Macro F1 Score")
plt.xlabel("Top-k Features")
plt.ylabel("Macro F1 Score")
plt.legend(title="Model")
plt.grid(True)
plt.tight_layout()
plt.show()

# Set color palette
sns.set_palette("tab10")

# Prepare AUC plot
plt.figure(figsize=(12, 6))

# Convert feature set names like 'Top2', 'Top4' to numeric values for x-axis
balanced_results_df['Num_Features'] = balanced_results_df['Feature_Set'].str.extract('(\d+)').astype(int)

# Plot AUC for each model
for model in balanced_results_df['Model'].unique():
    model_data = balanced_results_df[balanced_results_df['Model'] == model]
    plt.plot(model_data['Num_Features'], model_data['AUC'], marker='o', label=model)

plt.title("Top-k feature with AUC Score")
plt.xlabel("Top-k features")
plt.ylabel("AUC Score")
plt.legend(title="Model")
plt.grid(True)
plt.tight_layout()
plt.show()

# Highlight top 10 features
colors = ['tab:blue' if i >= 10 else 'tab:orange' for i in range(len(balanced_BordaCount_df))]

plt.figure(figsize=(12, 8))
plt.barh(balanced_BordaCount_df['Feature'], balanced_BordaCount_df['Borda_Total'], color=colors)
plt.title("Feature Importance")
plt.xlabel("Borda Count (Combined Rank)")
plt.ylabel("Features")
plt.gca().invert_yaxis()  # Highest score at the top
plt.tight_layout()
plt.show()

# Assuming X_train_final, y_train_final, X_val2, y_val2, and feature_sets["Top10"] are defined
# Define top10
top10 = feature_sets['Top10']

# Combine training data (X_train + X_val1) if not already done
X_train_concat = pd.concat([X_train, X_val1])
y_train_concat = pd.concat([y_train, y_val1])

# Apply hybrid undersample

from imblearn.pipeline import Pipeline
from imblearn.under_sampling import RandomUnderSampler, TomekLinks

pipeline = Pipeline([
    ('rus', RandomUnderSampler(random_state=42)),
    ('tl', TomekLinks())
])

X_train_final, y_train_final = pipeline.fit_resample(X_train_concat, y_train_concat)


# Dictionary to store best Optuna trials
best_trials = {}

# Objective function for Optuna tuning
def objective(trial, model_name):
    if model_name == 'LogisticRegression':
        C = trial.suggest_loguniform('C', 1e-3, 1e2)
        model = Pipeline([
            ('scaler', MinMaxScaler()),
            ('clf', LogisticRegression(C=C, class_weight='balanced', max_iter=1000))
        ])

    elif model_name == 'RandomForest':
        n_estimators = trial.suggest_int('n_estimators', 50, 300)
        max_depth = trial.suggest_int('max_depth', 2, 20)
        model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, class_weight='balanced', random_state=42)

    elif model_name == 'LightGBM':
        num_leaves = trial.suggest_int('num_leaves', 20, 150)
        max_depth = trial.suggest_int('max_depth', 3, 12)
        learning_rate = trial.suggest_float('learning_rate', 1e-3, 0.3, log=True)
        model = LGBMClassifier(num_leaves=num_leaves, max_depth=max_depth, is_unbalance=True,
                               learning_rate=learning_rate, n_estimators=100,
                               random_state=42)

    elif model_name == 'CatBoost':
        depth = trial.suggest_int('depth', 3, 10)
        learning_rate = trial.suggest_float('learning_rate', 1e-3, 0.3, log=True)
        model = CatBoostClassifier(depth=depth, learning_rate=learning_rate, class_weights=[1, 10, 5],
                                   n_estimators=100, verbose=0, random_state=42, task_type='CPU')

    elif model_name == 'ExtraTrees':
        n_estimators = trial.suggest_int('n_estimators', 50, 200)
        max_depth = trial.suggest_int('max_depth', 2, 20)
        model = ExtraTreesClassifier(n_estimators=n_estimators, max_depth=max_depth,
                                     class_weight='balanced', random_state=42)

    model.fit(X_train_final[top10], y_train_final)
    preds = model.predict_proba(X_val2[top10])
    auc = roc_auc_score(y_val2, preds, multi_class='ovr', average='weighted')
    return auc

# Run optimization for all models
for model_name in ['LogisticRegression', 'RandomForest', 'LightGBM', 'CatBoost', 'ExtraTrees']:
    print(f"Optimizing {model_name}...")
    study = optuna.create_study(direction='maximize')
    study.optimize(lambda trial: objective(trial, model_name), n_trials=30, show_progress_bar=True)
    best_trials[model_name] = study.best_trial
    print(f"Best AUC for {model_name}: {study.best_value:.4f}")
    print(f"Best Params: {study.best_params}")

# Display the best hyperparameters and AUC scores
for model_name, trial in best_trials.items():
    print(f"Model: {model_name}")
    print(f"  Best AUC Score : {trial.value:.4f}")
    print(f"  Best Parameters: {trial.params}")
    print("-" * 50)

test_results = []
def train_model(model_name, params):
    if model_name == 'LogisticRegression':
        model = Pipeline([
            ('scaler', MinMaxScaler()),
            ('clf', LogisticRegression(C=params['C'], max_iter=1000, class_weight='balanced', random_state=42))
        ])
    elif model_name == 'RandomForest':
        model = RandomForestClassifier(n_estimators=params['n_estimators'], class_weight='balanced',
                                       max_depth=params['max_depth'], random_state=42)
    elif model_name == 'LightGBM':
        model = LGBMClassifier(num_leaves=params['num_leaves'], max_depth=params['max_depth'], is_unbalance=True,
                               learning_rate=params['learning_rate'], n_estimators=100, random_state=42)
    elif model_name == 'CatBoost':
        model = CatBoostClassifier(depth=params['depth'], learning_rate=params['learning_rate'], class_weights=[1,10,5],
                                   n_estimators=100, verbose=0, random_state=42, task_type='CPU')
    elif model_name == 'ExtraTrees':
        model = ExtraTreesClassifier(n_estimators=params['n_estimators'], max_depth=params['max_depth'],
                                     class_weight='balanced', random_state=42)
    else:
        raise ValueError(f"Unsupported model: {model_name}")

    model.fit(X_train_final[top10], y_train_final)
    return model

def find_best_threshold(probs, y_true, class_index=1):
    class_probs = probs[:, class_index]
    precisions, recalls, thresholds = precision_recall_curve(y_true == class_index, class_probs)
    f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-8)
    best_idx = np.argmax(f1_scores)
    best_threshold = thresholds[best_idx] if best_idx < len(thresholds) else 1.0
    print(f"Best threshold for class {class_index}: {best_threshold:.3f} with F1 score: {f1_scores[best_idx]:.4f}")

    plt.figure(figsize=(8,6))
    plt.plot(thresholds, precisions[:-1], label='Precision')
    plt.plot(thresholds, recalls[:-1], label='Recall')
    plt.plot(thresholds, f1_scores[:-1], label='F1 Score')
    plt.axvline(best_threshold, color='r', linestyle='--', label='Best Threshold')
    plt.xlabel('Threshold')
    plt.ylabel('Score')
    plt.title(f'Precision, Recall, F1 vs Threshold for class {class_index}')
    plt.legend()
    plt.show()

    return best_threshold

def evaluate_model_with_optimal_threshold(model_name, params):
    model = train_model(model_name, params)
    pred_probs = model.predict_proba(X_test[top10])

    # Find best threshold for class 1
    best_thresh = find_best_threshold(pred_probs, y_test, class_index=1)

    # Apply threshold tuning to predictions
    preds_argmax = np.argmax(pred_probs, axis=1)
    preds_class1 = (pred_probs[:, 1] >= best_thresh).astype(int)
    preds = preds_argmax.copy()
    preds[preds_class1 == 1] = 1

    # Compute metrics
    acc = accuracy_score(y_test, preds)
    prec = precision_score(y_test, preds, average='weighted', zero_division=0)
    rec = recall_score(y_test, preds, average='weighted')
    f1 = f1_score(y_test, preds, average='weighted')
    auc = roc_auc_score(y_test, pred_probs, multi_class='ovr', average='weighted')
    cm = confusion_matrix(y_test, preds)
    report = classification_report(y_test, preds)

    print(f"Final evaluation for {model_name} at threshold {best_thresh:.3f}:")
    print(f"  Accuracy: {acc:.4f}")
    print(f"  Precision (weighted): {prec:.4f}")
    print(f"  Recall (weighted): {rec:.4f}")
    print(f"  F1 Score (weighted): {f1:.4f}")
    print(f"  AUC (weighted, OVR): {auc:.4f}")
    print("  Confusion Matrix:")
    print(cm)
    print("  Classification Report:")
    print(report)
    print("-" * 50)

    # Save results
    test_results.append({
        'Model': model_name,
        'Best Threshold': best_thresh,
        'Accuracy': acc,
        'Precision': prec,
        'Recall': rec,
        'F1 Score': f1,
        'AUC': auc
    })

# Run for all models in best_trials
for model_name, trial in best_trials.items():
    evaluate_model_with_optimal_threshold(model_name, trial.params)

# Store best trials of the model
with open('/content/drive/MyDrive/MRP/best_trials_hybridunder.pkl', 'wb') as f:
    pickle.dump(best_trials, f)

# Load best_trials from file
#with open('best_trials_hybridunder.pkl', 'rb') as f:
#    best_trials_t = pickle.load(f)

# Assuming results is a list of dicts collected during evaluation
HYBRIDunder_results_df = pd.DataFrame(test_results)

# Optionally, save to CSV for later use
HYBRIDunder_results_df.to_csv('/content/drive/MyDrive/MRP/hybridunder_evaluation_results.csv', index=False)